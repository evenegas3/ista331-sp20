{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Distances (and one more)\n",
    "\n",
    "This worksheet asks you to write Python code for three of the four distances covered in the lecture video: Manhattan distance, Jaccard similarity, and Kullback-Leibler divergence. It also includes a simple but inefficient implementation of Levenshtein distance for you to play around with.\n",
    "\n",
    "The **exercises** ask you to define the distance functions; **refinements** ask you to extend them in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manhattan distance\n",
    "\n",
    "*Manhattan distance*, aka *taxicab distance*, measures the distance between two vectors $\\mathbf u, \\mathbf v$ as\n",
    "\n",
    "$$ d_M(\\mathbf u, \\mathbf v) = \\sum_{i=0}^{m-1} |u_i - v_i| $$\n",
    "\n",
    "so, the sum of the absolute differences between coordinates. For instance, if $\\mathbf u = (3, -1)^T$ and $\\mathbf v = (0, -4)^T$, then the Manhattan distance between them is 6. In contrast, the usual Euclidean distance between these vectors is $\\sqrt{18}$, calculated by the formula:\n",
    "\n",
    "$$ d_E(\\mathbf u, \\mathbf v) = \\sqrt{\\sum_{i=0}^{m-1} (u_i - v_i)^2 } $$\n",
    "\n",
    "The Manhattan distance is sometimes called the $L^1$ norm (usually this terminology appears in more math-focused sources). In this language the Euclidean distance is the $L^2$ norm, and there is in fact an entire family of $L^p$ norms defined by\n",
    "\n",
    "$$ d_p(\\mathbf u, \\mathbf v) = \\left( \\sum_{i=0}^{m-1} |u_i - v_i|^p \\right)^{1/p} $$\n",
    "\n",
    "Although this defines a valid distance for any value of $p$ greater than or equal to 1, the values $p = 1, 2, \\infty$ are the most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function called `manhattan_distance` that computes the Manhattan ($L^1$) distance between two numerical vectors (which can be `numpy` arrays or `Series`, for instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(u, v):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the Manhattan distance\n",
    "\n",
    "The following code plots contours of the Euclidean distance; that is, curves where every point is equal distance from the origin. Of course, you know what these curves should look like: a curve where every point is equally far from a single center point is a circle.\n",
    "\n",
    "After running the code below to see what it does, try to imagine what the contours would look like if we measured with Manhattan distance. In other words, is an $L^1$ circle still a circle? After you've thought about this, replace `euclidean_distance` in the plotting code with your `manhattan_distance` function and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(u, v):\n",
    "    return np.sqrt(np.sum((u - v) ** 2))\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(-5, 5, 0.1), np.arange(-5, 5, 0.1))\n",
    "Z = np.array([euclidean_distance(u, np.array([0., 0.])) for u in np.c_[xx.ravel(), yy.ravel()]])\n",
    "Z = Z.reshape(yy.shape)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.contour(xx, yy, Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refinement\n",
    "\n",
    "Write a function called `p_norm` that takes two vectors and a float `p` (which should be positive) and returns the $L^p$ distance between them. Try this in the plotting code for a few different values of `p`, and observe how the contours change. What happens for very large values of `p` (i.e. as $p \\to \\infty$)? What about `p` near 0? (When $p < 1$, this does not follow all of the mathematical rules for a distance function, but the formula still makes sense, and the $L^0$ \"distance\" does in fact have some applications in data analysis.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard similarity\n",
    "\n",
    "The Jaccard similarity measures similarity between two binary feature vectors, meaning vectors of 0s and 1s. These represent vectors of attributes that an instance either has or doesn't have. \n",
    "\n",
    "For example, we may describe various cars by the presence or absence of certain features:\n",
    "\n",
    "| **model** | **car** | **truck** | **SUV** | **hybrid** | **diesel** | **plug-in** | **AWD** | **efficient** |\n",
    "|-------|---|---|---|---|---|---|---|---|\n",
    "| prius | 1 | 0 | 0 | 1 | 0 | 1 | 0 | 1 |\n",
    "| tesla | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |\n",
    "| f-150 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 |\n",
    "\n",
    "On the surface, we can see the `tesla` is more similar to the `prius` because it has more features in common.\n",
    "\n",
    "The Jaccard similarity measures this as:\n",
    "\n",
    "$$ JS(\\mathbf u, \\mathbf v) = \\frac{\\mbox{# attributes both have}}{\\mbox{# attributes either has}} = \\frac{|\\mathbf u \\cap \\mathbf v|}{|\\mathbf u \\cup \\mathbf v|}$$\n",
    "\n",
    "In the cosine similarity notebook, when we were examining the data about visitors to `microsoft.com`, we calculated almost the same thing, because the attributes were all of the form \"visited/didn't visit some area of the website\". (The denominator in cosine similarity is somewhat different, though.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function called `jaccard` that takes two binary feature vectors (in the form of `Series`) and returns the Jaccard similarity between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(u, v):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refinement\n",
    "\n",
    "Rewrite the `jaccard` function to deal with `Series` that don't necessarily have all of the same attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler divergence is a way of measuring \"distance\" between two probability distributions with the same outcomes. If $P = (p_0, p_1, \\ldots, p_{m-1})$ and $Q = (q_0, q_1, \\ldots, q_{m-1}$, then the K-L divergence from $Q$ to $P$ is\n",
    "\n",
    "$$ D_{KL} (P || Q) = -\\sum_{i=0}^{m-1} p_i \\log_2 \\frac{q_i}{p_i} $$\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$$ D_{KL} (P || Q) = \\sum_{i=0}^{m-1} p_i \\log_2 \\frac{p_i}{q_i} $$\n",
    "\n",
    "This formula is a bit opaque at first glance, but it is closely related to some information-theoretic quantities that we will use when discussing decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function called `kld` that takes two probability distributions `p` and `q` (in the form of `Series`) and computes the Kullback-Leibler divergence from `q` to `p`. Use `np.log2` to compute the base-2 log. You can assume that the `Series` are the same length with the same indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kld(p, q):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $P = (0.3, 0.2, 0.1, 0.2, 0.2)$ and $Q = (0.1, 0.1, 0.05, 0.05, 0.7)$. Use your function above to calculate $D_{KL}(P || Q)$ and $D_{KL}(Q || P)$. Are they equal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein distance\n",
    "\n",
    "Levenshtein distance is a way to calculate a distance between two strings. It is defined to be the minimum number of single-character insertions, deletions, or substitutions required to get from one string to another.\n",
    "\n",
    "For example, the Levenshtein distance between `python` and `psycho` is 3, because the fastest way to edit one word to the other is:\n",
    "\n",
    "    python\n",
    "    psython\n",
    "    psychon\n",
    "    psycho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation of this distance is below. Notice that this is a recursive function -- it calls itself. This makes it easy to express, but it's quite inefficient. There are better ways to compute this if you want to handle more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s, l):\n",
    "    if not s:\n",
    "        return len(l)\n",
    "    if not l:\n",
    "        return len(s)\n",
    "    if s[0] == l[0]:\n",
    "        return levenshtein(s[1:], l[1:])\n",
    "    else:\n",
    "        return 1 + min(levenshtein(s, l[1:]), levenshtein(s[1:], l), levenshtein(s[1:], l[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try calling this on a few strings below. It's probably best to stick with single words; the recursive implementation above can get very slow on longer strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein('kangaroo', 'anger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
