{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from matplotlib import cm\n",
    "# Uncomment the following once you have installed graphviz (both the Python module and the package itself)\n",
    "# https://www.graphviz.org/download/\n",
    "# from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Nonparametric model__: <font color = 'red'> **A model where the parameters (number, meaning) are not fixed before training. For example, in a curve fit, we specify the model function, coefficients, and what the coefficients mean before doing anything else. In a nonparametric model, we don't know the full structure of the model in advance.**\n",
    "    \n",
    "**This can offer a lot of flexibility, but can also increase the variance of our models and the risk of overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Entropy:__ <font color = 'red'> **A measure of uncertainty applied to a probability distribution. Calculated using the formula**\n",
    "\n",
    "<font color = 'black'>\n",
    "$$ H(X) = -\\sum_i p_i \\log_2(p_i) $$\n",
    "\n",
    "<font color = 'red'>\n",
    "    \n",
    "**where $p_i$ are the probabilities of the outcomes/values $x_i$ of $X$. It's named for an analogous concept in statistical physics/thermodynamics, which measures the disorder in a collection of particles using a similar mathematical function. Originated in the theory of communication and message encoding developed by Claude Shannon and others (which is why you sometimes hear this called Shannon entropy) at Bell Labs in the 1940s and later.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Conditional entropy:__ <font color = 'red'> **Like probabilities, we can update the calculation of entropy when we observe a related variable. If the observed variable is categorical, we can think of it as breaking the data into subsets. We calculate the entropy of the target variable within each subset normally, then take a weighted average according to the frequency of each subset.**\n",
    "    \n",
    "<font color = 'black'> \n",
    "$$ H(X | Y) = \\sum_j P(\\mbox{subset j}) H(\\mbox{subset j}) $$\n",
    "\n",
    "<font color = 'red'>     \n",
    "**or, more formally (this is what you'll probably see if you look the definition up):**\n",
    "\n",
    "<font color = 'black'> \n",
    "$$ H(X | Y) = \\sum_j P(Y = y_j) H(X | Y = y_j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Mutual information:__ <font color = 'red'> **If $X$ and $Y$ are two variables, the mutual information is**\n",
    "    \n",
    "<font color = 'black'>\n",
    "$$I(X; Y) = H(X) - H(X | Y)$$\n",
    "    \n",
    "<font color = 'red'> \n",
    "    \n",
    "**or the \"gap\" between the entropy of a variable, and the conditional entropy after observing a related variable. Since it measures how much uncertainty is reduced by the observation, this is also sometimes called \"information gain.\" Note $I(X; Y) = I(Y; X)$ even though $H(X | Y) \\neq H(Y | X)$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Cost function / benefit function:__ <font color = 'red'> **A cost function (also called a loss function) is a measure of goodness of fit that we want to minimize -- i.e. something that measures the error in our model. Example: sum of squared residuals in curve fitting. A benefit function is the opposite: a measure of goodness of fit that we try to maximize. Here, mutual information is serving as a benefit function.**\n",
    "    \n",
    "**Note you can generally translate between cost / benefit functions by multiplying by -1; this is sometimes done for computational or mathematical convenience.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for estimating the entropy of a target variable from a data frame.\n",
    "\n",
    "def entropy(df, target):\n",
    "    '''\n",
    "    Computes the entropy of the target variable in the data frame.\n",
    "    Parameters:\n",
    "        df: a DataFrame containing labeled data with a categorical target variable\n",
    "        target: the name of the target variable (str)\n",
    "    Returns:\n",
    "        H, the entropy of the target variable (float)\n",
    "    '''\n",
    "    labels = {label for label in df[target]}\n",
    "    probs = [len(df[df[target] == label]) / len(df) for label in labels]\n",
    "    return -sum(p * np.log2(p) for p in probs if p != 0)\n",
    "\n",
    "# Utility function for estimating conditional entropy from a data frame.\n",
    "\n",
    "def cond_entropy(df, pred, target):\n",
    "    '''\n",
    "    Computes the conditional entropy of a target variable given a certain predictor.\n",
    "    Parameters:\n",
    "        df: a DataFrame containing labeled data with categorical predictors and a categorical target\n",
    "        pred: the name of the attribute being used as a predictor (str)\n",
    "        target: the name of the target variable (str)\n",
    "    Returns:\n",
    "        H, the conditional entropy (float)\n",
    "    '''\n",
    "    # These are set comprehensions. Like list comprehensions but they make sets.\n",
    "    # We use sets so that there are no duplicates; we want to consider each category/label only once\n",
    "    categories = {cat for cat in df[pred]}\n",
    "    labels = {label for label in df[target]}\n",
    "    # Make a list of subsets of the data frame, broken down by the predictor\n",
    "    subsets = [df[df[pred] == cat] for cat in categories]\n",
    "    H = 0\n",
    "    for subset in subsets:\n",
    "        if len(subset) > 0: # avoid some errors\n",
    "            # Calculate the label probabilities within the subset, put them in a list\n",
    "            probs = [len(subset[subset[target] == label]) / len(subset) for label in labels]\n",
    "            # Calculate the contribution of this subset to the conditional entropy and add it to H\n",
    "            H += (len(subset) / len(df)) * (-sum(p * np.log2(p) for p in probs if p != 0))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the 60s on survival after surgery for breast cancer\n",
    "surv = pd.read_csv('survival.csv')\n",
    "surv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data frame, a `Survival` value of 1 indicates survival 5+ years, 2 indicates death within 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(surv, 'Survival')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best variable to split on\n",
    "\n",
    "We want to formulate a decision tree as a sequence of yes/no questions about the predictors. If the predictors are categorical, especially binary categorical, it is simple to formulate the questions.\n",
    "\n",
    "All of our predictors in this data set are numerical, though. So our questions should look like:\n",
    "* is age > 35?\n",
    "* is nodes = 0?\n",
    "* etc.\n",
    "\n",
    "At each step we split the data into two pieces, and eventually make an estimate based on that.\n",
    "\n",
    "There are many such questions we can ask at any point. How do we pick one? Look for the best mutual information/information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Write a function called `mutual_information` that takes a data frame, a predictor variable, a cutoff value for that predictor variable, and a target variable. The function returns the mutual information of the target variable and the binary categorical variable `predictor > / <= cutoff`.\n",
    "\n",
    "**Hint:** create a temporary copy of the data frame, expand it by adding a new column representing the binary categorical variable, then use the utility functions `entropy` and `conditional_entropy` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(df, predictor, cutoff, target):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(set(surv['Age'])), \n",
    "         [mutual_information(surv, 'Age', x, 'Survival') for x in sorted(set(surv['Age']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot tells us that if we ask about age, the question should be \"is `age > 40`?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(set(surv['Year'])), \n",
    "         [mutual_information(surv, 'Year', x, 'Survival') for x in sorted(set(surv['Year']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the year has very little mutual information with the survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(set(surv['Nodes'])), \n",
    "         [mutual_information(surv, 'Nodes', x, 'Survival') for x in sorted(set(surv['Nodes']))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can settle on something: the best first question to ask is: is `nodes > 4`? In fact, a 1983 study [https://www.ncbi.nlm.nih.gov/pubmed/6352003] groups patients into 0 nodes, 1-3 nodes, and 4+ nodes, so our approach suggests a grouping pretty similar to that, which is encouraging.\n",
    "\n",
    "Let's see what the survival rates are in these two subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_a = surv[surv['Nodes'] <= 4]\n",
    "surv_b = surv[surv['Nodes'] > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(surv_a['Survival'] == 1) / len(surv_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(surv_b['Survival'] == 1) / len(surv_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Apply the same approach as above to split each of the data frames `surv_a` and `surv_b` into two. (Think about which variables you might want to use for a second split, then plot their mutual information with Survival. Choose a best variable and a cutoff.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make our estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you have graphviz working\n",
    "'''\n",
    "tree = Digraph()\n",
    "tree.node('top', 'Start')\n",
    "tree.node('A', 'Nodes <= 4')\n",
    "tree.node('B', 'Nodes > 4')\n",
    "tree.node('AA', 'Age <= 59')\n",
    "tree.node('AB', 'Age > 59')\n",
    "tree.node('BA', 'Age <= 63')\n",
    "tree.node('BB', 'Age > 63')\n",
    "\n",
    "tree.node('end_AA', 'Prognosis: 0.835')\n",
    "tree.node('end_AB', 'Prognosis: 0.772')\n",
    "tree.node('end_BA', 'Prognosis: 0.507')\n",
    "tree.node('end_BB', 'Prognosis: 0.286')\n",
    "\n",
    "tree.edge('top', 'A')\n",
    "tree.edge('top', 'B')\n",
    "tree.edge('A', 'AA')\n",
    "tree.edge('A', 'AB')\n",
    "tree.edge('B', 'BA')\n",
    "tree.edge('B', 'BB')\n",
    "\n",
    "tree.edge('AA', 'end_AA')\n",
    "tree.edge('AB', 'end_AB')\n",
    "tree.edge('BA', 'end_BA')\n",
    "tree.edge('BB', 'end_BB')\n",
    "\n",
    "tree\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A conceptual digression\n",
    "\n",
    "From a more theoretical perspective, what a tree does is:\n",
    "* partition the space of predictors into subsets\n",
    "* fit a very simple model (a constant) on each subset\n",
    "\n",
    "Let's visualize the prognosis as a function of age and nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make data\n",
    "Y = np.arange(25, 85, 0.25)\n",
    "X = np.arange(0, 40, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "# Quick and dirty prediction function\n",
    "def predict(nodes, age):\n",
    "    if nodes <= 4:\n",
    "        if age <= 59:\n",
    "            return 0.835\n",
    "        else:\n",
    "            return 0.772\n",
    "    else:\n",
    "        if age <= 63:\n",
    "            return 0.507\n",
    "        else:\n",
    "            return 0.286\n",
    "\n",
    "Z = np.array([predict(x, y) for [x,y] in np.c_[X.ravel(), Y.ravel()]])\n",
    "Z = Z.reshape(Y.shape)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0.1, edgecolor = 'black', antialiased=True)\n",
    "\n",
    "ax.set_xlabel('Nodes')\n",
    "ax.set_ylabel('Age')\n",
    "ax.set_zlabel('Prognosis')\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the shape of the flat regions. Because of the way we structure our tree, these areas are always rectangles -- i.e., decision boundaries are always parallel to a coordinate axis. This is a limitation of these trees. In theory, any shape of decision boundary could be approximated by these rectangles, but that would mean a very complex tree and a high risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, but how good is this really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We committed a sin by using our entire data set to build the model instead of holding out some data for validation. Next time, we repent our crimes. We'll discover some of the weaknesses of decision trees and how they may be fixed in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
