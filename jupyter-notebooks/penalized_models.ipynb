{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to penalized regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Regularization:__ <font color = 'black'> Adding a constraint or a penalty to a model to reduce its complexity. We did this with trees by setting a maximum depth. The purpose of this is to reduce overfitting, by not allowing a model to freely expand its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Bias-variance tradeoff:__ <font color = 'black'> Prediction error can be roughly broken down into\n",
    "* irreducible error, coming from inherent variability in the target variable; we can't do anything about this\n",
    "* bias, coming from a model that is insufficient to capture the relationships between the predictors and the target\n",
    "* variance, coming from sensitivity of the model to variations in the training data\n",
    "    \n",
    "Overfitting error is due to excess variance. Generally speaking, more complex models often have lower bias but higher variance. This is the *bias-variance tradeoff.* We can exploit it by introducing a small amount of bias to reduce variance -- this is the goal of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a data set of observations of 252 people's body dimensions, along with measurements of their body fat percentage. Our goal is to estimate body fat from the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bodyfat = pd.read_csv('bodyfat.csv')\n",
    "print(bodyfat.shape)\n",
    "bodyfat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Cost function:__ <font color = 'black'> A cost function or loss function measures the inaccuracy of a model. Curve fitting models, including linear regression, operate by finding coefficients of the model so that the cost function is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Linear model:__ <font color = 'black'> For our purposes, a linear model is one with a model equation of the form\n",
    "\n",
    "$$ \\hat y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_m x_m $$\n",
    "    \n",
    "or, written as a vector equation,\n",
    "\n",
    "$$ \\hat y = \\boldsymbol \\beta \\cdot \\mathbf x $$\n",
    "\n",
    "where $\\boldsymbol \\beta = (\\beta_0, \\beta_1, \\ldots)$ and $\\mathbf x = (1, x_1, x_2, \\ldots)$ (the 1 is to get the constant term / intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Least squares:__ <font color = 'black'> In least squares, we use the cost function\n",
    "\n",
    "$$ \\mathcal L(\\boldsymbol \\beta) = \\frac{1}{N} \\sum_i |y_i - \\hat y_i|^2 $$\n",
    "\n",
    "(i.e. the mean squared error)\n",
    "    \n",
    "The script $\\mathcal L$ is common notation for a cost function (from the other common term for a cost function, *loss function*). Note that we think of $\\mathcal L$ as a function of the coefficients $\\mathbf \\beta$, not so much as a function of $\\mathbf x$ or $y$. This is because we want to find the value of $\\mathbf \\beta$ that minimizes the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Ridge regression:__ <font color = 'black'> A modification of least squares, using the loss function\n",
    "    \n",
    "$$ \\mathcal L_{ridge}(\\boldsymbol \\beta) = \\frac{1}{N} \\sum_i |y_i - \\hat y_i|^2 + \\alpha \\|\\boldsymbol \\beta\\|^2$$\n",
    "\n",
    "where the double vertical bars $\\| \\cdot \\|$ refer to vector magnitude. So, in ridge regression, we \"penalize\" models that have large coefficients by adding the squares of those coefficients to the cost. The result is a model that tries to minimize the MSE while also keeping the coefficients small. The tuning parameter (hyperparameter) $\\alpha$ control the balance between these two goals. The larger $\\alpha$ is, the more the model will prioritize small coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Lasso:__ <font color = 'black'> A modification of least squares, using the loss function\n",
    "    \n",
    "$$ \\mathcal L_{lasso}(\\boldsymbol \\beta) = \\frac{1}{N} \\sum_i |y_i - \\hat y_i|^2 + \\alpha \\sum_j |\\beta_j|$$\n",
    "    \n",
    "so we are penalizing the absolute values of the coefficients instead of their squares.\n",
    "    \n",
    "This seems like a small difference, but the lasso method has one important property. Ridge regression will never force any of the coefficients all the way to 0, but the lasso can. Since setting a coefficient to 0 amounts to dropping that predictor for the model, the lasso method is capable of performing *variable selection*; it can suggest which predictors, if any, should be excluded from the model entirely. This can be useful when we have a large number of predictors and cannot easily tell which are the most useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Elastic net:__ <font color = 'black'> A model that combines the lasso with ridge regression by using the cost function\n",
    "    \n",
    "$$ \\mathcal L_{net}(\\boldsymbol \\beta) = t \\cdot \\mathcal L_{lasso}(\\boldsymbol \\beta) + (1-t) \\cdot \\mathcal L_{ridge}(\\boldsymbol \\beta)$$\n",
    "    \n",
    "$0 \\leq t \\leq 1$ is another tuning parameter, controlling the mixture of the two penalties. Notice that when $t = 1$, we just have a lasso; when $t = 0$, we just have a ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a few examples of the above models to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select out a training set\n",
    "\n",
    "train_idx = np.random.choice(bodyfat.index, 50, replace = False)\n",
    "bodyfat_train = bodyfat.loc[train_idx]\n",
    "bodyfat_test = bodyfat.drop(train_idx)\n",
    "train_X = bodyfat_train.drop('bodyfat', axis = 1)\n",
    "train_y = bodyfat_train['bodyfat']\n",
    "test_X = bodyfat_test.drop('bodyfat', axis = 1)\n",
    "test_y = bodyfat_test['bodyfat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit a few of these models to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = LinearRegression()\n",
    "ridge_model = Ridge(alpha = 1)\n",
    "lasso_model = Lasso(alpha = 0.01)\n",
    "net_model = ElasticNet(alpha = 0.5)\n",
    "\n",
    "ols_model.fit(train_X, train_y)\n",
    "ridge_model.fit(train_X, train_y)\n",
    "lasso_model.fit(train_X, train_y)\n",
    "net_model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of $\\alpha$ are chosen above to conveniently illustrate some features of the models, but in practice you'd want to select these using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_preds = ols_model.predict(test_X)\n",
    "ridge_preds = ridge_model.predict(test_X)\n",
    "lasso_preds = lasso_model.predict(test_X)\n",
    "net_preds = net_model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OLS RMSE:\", np.sqrt(mean_squared_error(ols_preds, test_y)))\n",
    "print(\"Ridge RMSE:\", np.sqrt(mean_squared_error(ridge_preds, test_y)))\n",
    "print(\"Lasso RMSE:\", np.sqrt(mean_squared_error(lasso_preds, test_y)))\n",
    "print(\"Elastic net RMSE:\", np.sqrt(mean_squared_error(net_preds, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the lasso performed the best here. Let's take a look at the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OLS coefficients:\")\n",
    "print(ols_model.coef_)\n",
    "print(\"Ridge coefficients:\")\n",
    "print(ridge_model.coef_)\n",
    "print(\"Lasso coefficients:\")\n",
    "print(lasso_model.coef_)\n",
    "print(\"Elastic net coefficients:\")\n",
    "print(net_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the lasso dropped a number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_X.columns[i] for i in range(len(train_X.columns)) if lasso_model.coef_[i] != 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_X.columns[i] for i in range(len(train_X.columns)) if lasso_model.coef_[i] == 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
