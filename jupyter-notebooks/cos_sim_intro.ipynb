{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Cosine similarity:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Similarity of website visitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Sparse vector:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('msweb.pkl', 'rb') as fp:\n",
    "    msweb = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is anonymized data of visitors to `microsoft.com` collected over one week in February 1998. Each key in the dictionary is an ID assigned to a visitor; each value is a list of website areas that user visited during the one-week period.\n",
    "\n",
    "Each of these lists can be regarded as a vector in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = set()\n",
    "for idnum in msweb:\n",
    "    areas = areas.union(set(msweb[idnum]))\n",
    "len(areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average([len(l) for l in msweb.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors are quite sparse: the average user visited about 3 website areas out of the almost 300 areas available. So, if we compare entry-by-entry, most vectors will look pretty similar, because almost all entries for both vectors will be 0.\n",
    "\n",
    "Cosine similarity only considers coordinates where at least one vector has a nonzero entry -- this makes it good for handling vectors in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(id1, id2):\n",
    "    dot = len(set.intersection(set(msweb[id1]), set(msweb[id2])))\n",
    "    return dot / (np.sqrt(len(msweb[id1])) * np.sqrt(len(msweb[id2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([cos_sim('10190', id) for id in msweb.keys()], bins = np.linspace(0, 1, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([int(k) for k in msweb.keys()],\n",
    "         [cos_sim('10190', k) for k in sorted(msweb.keys(), key = lambda x:cos_sim('10190', x))], ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Example extension:__ <font color = 'red'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Similarity of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Bag of words:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Stop words:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Stemming:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @realDonaldTrump tweets, not including retweets/likes, from 1/21/2017 to 3/30/2020\n",
    "with open('trump_tweets.json') as fp:\n",
    "    tweets = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What devices did tweets come from?\n",
    "set([tweet['source'] for tweet in tweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we tell the difference, from a data mining perspective, between tweets from different devices? Let's look at iPhone vs. Android."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_tweets = [tw for tw in tweets if tw['source'] == 'Twitter for iPhone']\n",
    "android_tweets = [tw for tw in tweets if tw['source'] == 'Twitter for Android']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A somewhat modified version of vectorize() from HW5\n",
    "def vectorize(text, stop, stemmer):\n",
    "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    counts = {}\n",
    "    for word in words:\n",
    "        word = stemmer.stem(word)\n",
    "        if word not in stop:\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem: an individual tweet doesn't contain enough words to be a good basis for comparison. Let's pool the contents of a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_sample = random.sample(iphone_tweets, 50)\n",
    "android_sample = random.sample(android_tweets, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_text = ' '.join([tw['text'] for tw in iphone_sample])\n",
    "android_text = ' '.join([tw['text'] for tw in android_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = vectorize(iphone_text, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most common words?\n",
    "sorted(vec, key = lambda k:vec[k], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __What data cleaning should we consider?__ <font color = 'red'> **Looking toward the end, we see some links, which should be removed; possibly keywords such as RT (although what's the potential problem here?); possibly hashtags; possibly numbers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_a = vectorize(android_text, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most common words?\n",
    "sorted(vec_a, key = lambda k:vec_a[k], reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we notice any differences between the words that appear most often in each sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(u, v):\n",
    "    total = 0\n",
    "    for word in u:\n",
    "        if word in v:\n",
    "            total += u[word] * v[word]\n",
    "    return total\n",
    "\n",
    "def magnitude(u):\n",
    "    return np.sqrt(dot_product(u, u))\n",
    "\n",
    "def cosine_similarity(u,v):\n",
    "    return dot_product(u, v) / (magnitude(u) * magnitude(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_vec = vectorize(iphone_text, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "android_vec = vectorize(android_text, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "cosine_similarity(iphone_vec, android_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iphone_sample_1 = random.sample(iphone_tweets, 50)\n",
    "android_sample_1 = random.sample(android_tweets, 50)\n",
    "iphone_sample_2 = random.sample(iphone_tweets, 50)\n",
    "android_sample_2 = random.sample(android_tweets, 50)\n",
    "\n",
    "iphone_text_1 = ' '.join([tw['text'] for tw in iphone_sample_1])\n",
    "android_text_1 = ' '.join([tw['text'] for tw in android_sample_1])\n",
    "iphone_text_2 = ' '.join([tw['text'] for tw in iphone_sample_2])\n",
    "android_text_2 = ' '.join([tw['text'] for tw in android_sample_2])\n",
    "\n",
    "ivec_1 = vectorize(iphone_text_1, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "ivec_2 = vectorize(iphone_text_2, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "avec_1 = vectorize(android_text_1, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "avec_2 = vectorize(android_text_2, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cosine_similarity(ivec_1, avec_1))\n",
    "print(cosine_similarity(ivec_2, avec_2))\n",
    "print(cosine_similarity(ivec_1, ivec_2))\n",
    "print(cosine_similarity(avec_1, avec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.zeros((200, 4))\n",
    "for i in range(200):\n",
    "    \n",
    "    iphone_sample_1 = random.sample(iphone_tweets, 50)\n",
    "    android_sample_1 = random.sample(android_tweets, 50)\n",
    "    iphone_sample_2 = random.sample(iphone_tweets, 50)\n",
    "    android_sample_2 = random.sample(android_tweets, 50)\n",
    "\n",
    "    iphone_text_1 = ' '.join([tw['text'] for tw in iphone_sample_1])\n",
    "    android_text_1 = ' '.join([tw['text'] for tw in android_sample_1])\n",
    "    iphone_text_2 = ' '.join([tw['text'] for tw in iphone_sample_2])\n",
    "    android_text_2 = ' '.join([tw['text'] for tw in android_sample_2])\n",
    "\n",
    "    ivec_1 = vectorize(iphone_text_1, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "    ivec_2 = vectorize(iphone_text_2, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "    avec_1 = vectorize(android_text_1, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "    avec_2 = vectorize(android_text_2, stop_words.ENGLISH_STOP_WORDS, SnowballStemmer(\"english\"))\n",
    "    \n",
    "    similarities[i, 0] = cosine_similarity(ivec_1, avec_1)\n",
    "    similarities[i, 1] = cosine_similarity(ivec_2, avec_2)\n",
    "    similarities[i, 2] = cosine_similarity(ivec_1, ivec_2)\n",
    "    similarities[i, 3] = cosine_similarity(avec_1, avec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3,1, figsize = (6, 12))\n",
    "ax1.hist(np.concatenate((similarities[:,0], similarities[:,1])), bins = np.arange(0.2, 0.875, 0.025), edgecolor = 'black')\n",
    "ax1.set_title('Similarities: iPhone vs. Android')\n",
    "ax2.hist(similarities[:,2], bins = np.arange(0.2, 0.875, 0.025), edgecolor = 'black')\n",
    "ax2.set_title('Similarities: iPhone vs. iPhone')\n",
    "ax3.hist(similarities[:,3], bins = np.arange(0.2, 0.875, 0.025), edgecolor = 'black')\n",
    "ax3.set_title('Similarities: Android vs. Android')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
