{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and the $k$-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Supervised learning:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Unsupervised learning:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Clustering:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: pets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = pd.read_csv('pets.csv', index_col=0)\n",
    "pets\n",
    "plt.plot(pets.loc[:,'weight'], pets.loc[:,'height'], 'o')\n",
    "plt.title('Pets by height, weight')\n",
    "plt.ylabel('Height (cm)')\n",
    "plt.xlabel('Weight (kg)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't know in advance that these are dogs, ponies, etc., we can still clearly see that the points naturally fall into two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_unlabeled = pets.drop('label', axis = 1)\n",
    "cls = KMeans(n_clusters=2)\n",
    "cls.fit(X=pets_unlabeled.values)\n",
    "labels = cls.predict(pets_unlabeled.values)\n",
    "\n",
    "cluster_0 = pets_unlabeled.loc[labels == 0].values\n",
    "cluster_1 = pets_unlabeled.loc[labels == 1].values\n",
    "plt.plot(cluster_0[:,1], cluster_0[:,0], 'o')\n",
    "plt.plot(cluster_1[:,1], cluster_1[:,0], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we had to specify the number of groups we were looking for. $k$, the number of clusters, is a hyperparameter -- we have to select it before training the model. (This is similar to the situation in $k$-nearest neighbors, but be careful -- these are not the same $k$!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_unlabeled = pets.drop('label', axis = 1)\n",
    "cls = KMeans(n_clusters=3)\n",
    "cls.fit(X=pets_unlabeled.values)\n",
    "labels = cls.predict(pets_unlabeled.values)\n",
    "\n",
    "cluster_0 = pets_unlabeled.loc[labels == 0].values\n",
    "cluster_1 = pets_unlabeled.loc[labels == 1].values\n",
    "cluster_2 = pets_unlabeled.loc[labels == 2].values\n",
    "plt.plot(cluster_0[:,1], cluster_0[:,0], 'o')\n",
    "plt.plot(cluster_1[:,1], cluster_1[:,0], 'o')\n",
    "plt.plot(cluster_2[:,1], cluster_2[:,0], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This successfully isolates the ponies, but can't perfectly separate the cats and dogs. But this is not unexpected -- we saw that can be hard even with labeled data.\n",
    "\n",
    "What if we increase the number of clusters even more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell a few times, see if anything changes\n",
    "pets_unlabeled = pets.drop('label', axis = 1)\n",
    "cls = KMeans(n_clusters=5)\n",
    "cls.fit(X=pets_unlabeled.values)\n",
    "labels = cls.predict(pets_unlabeled.values)\n",
    "\n",
    "cluster_0 = pets_unlabeled.loc[labels == 0].values\n",
    "cluster_1 = pets_unlabeled.loc[labels == 1].values\n",
    "cluster_2 = pets_unlabeled.loc[labels == 2].values\n",
    "cluster_3 = pets_unlabeled.loc[labels == 3].values\n",
    "cluster_4 = pets_unlabeled.loc[labels == 4].values\n",
    "plt.plot(cluster_0[:,1], cluster_0[:,0], 'o')\n",
    "plt.plot(cluster_1[:,1], cluster_1[:,0], 'o')\n",
    "plt.plot(cluster_2[:,1], cluster_2[:,0], 'o')\n",
    "plt.plot(cluster_3[:,1], cluster_3[:,0], 'o')\n",
    "plt.plot(cluster_4[:,1], cluster_4[:,0], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the algorithm will not find the same clusters every time. We'll see why this is in the lecture videos over the weekend, when we go over how the algorithm actually works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Preprocessing:__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some fake data\n",
    "x = 10*np.random.randn(1000)\n",
    "y = np.concatenate((np.random.randn(500), np.random.randn(500) + 5))\n",
    "df = pd.DataFrame(np.column_stack((x,y)), columns = ['x', 'y'])\n",
    "plt.plot(df['x'], df['y'], 'o')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data we can see two obvious clusters of points. So we let our $k$-means algorithm try to find them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what clusters the algorithm found.\n",
    "cluster_0 = df.loc[np.where(labels == 0)].values\n",
    "cluster_1 = df.loc[np.where(labels == 1)].values\n",
    "plt.plot(cluster_0[:,0], cluster_0[:,1], 'o')\n",
    "plt.plot(cluster_1[:,0], cluster_1[:,1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not good! Instead of detecting the two clusters that seem obvious to our eyes, it has split them both in half. Why?\n",
    "\n",
    "$k$-means, even though it's not quite a nearest-neighbor algorithm, is still based on distances: it classifies observations by their distance from an average point. The problem here is that the $y$ variable is the most informative variable; but, the $x$-coordinates vary on a much wider scale. So distances are dominated by distances in the $x$-direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try scaling the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what clusters the algorithm found.\n",
    "cluster_0 = frame.loc[np.where(labels == 0)].values\n",
    "cluster_1 = frame.loc[np.where(labels == 1)].values\n",
    "plt.plot(cluster_0[:,0], cluster_0[:,1], 'o')\n",
    "plt.plot(cluster_1[:,0], cluster_1[:,1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!\n",
    "\n",
    "Although this example was with artificial data, the problem is real. Most real data is measured in some kind of units, and changing the units (e.g. going from meters to cm) can drastically change the scales of different variables. This can negatively impact the performance of $k$-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: unknown wine\n",
    "\n",
    "The following data is from a chemical analysis of wines grown in Italy. The wines all come from the same region, but from three different cultivars (types of grape).\n",
    "\n",
    "Can we cluster the wines correctly so that we know which wines came from the same cultivar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('wine_noclass.csv')        # the wine data without labels\n",
    "labels = pd.read_csv('wine.data')['Class']    # the correct labels, for later comparison\n",
    "wine.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of variables; let's pick a few to plot against one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (14, 6))\n",
    "ax1.plot(wine['Alcohol'], wine['Malic Acid'], 'o')\n",
    "ax1.set_xlabel('Alcohol')\n",
    "ax1.set_ylabel('Malic Acid')\n",
    "ax2.plot(wine['Ash'], wine['Magnesium'], 'o')\n",
    "ax2.set_xlabel('Ash')\n",
    "ax2.set_ylabel('Magnesium')\n",
    "ax3.plot(wine['Proanthocynanins'], wine['Color intensity'], 'o')\n",
    "ax3.set_xlabel('Proanthocyanins')\n",
    "ax3.set_ylabel('Color intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three clusters are now not so easily distinguishable by eye. So, we can benefit by using some ML to try to extract them. Let's try running $k$-means on this data set. We know in advance that there should be three groups, so we can choose $k = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = KMeans(n_clusters=3) \n",
    "cls.fit(X = wine.values) \n",
    "clusters = cls.predict(wine.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a plot to compare the true labels to the found labels\n",
    "ax = plt.figure(figsize = (12, 4))\n",
    "true_clusters = [wine.index[labels == i] for i in range(1,4)]     # in the data set labels are 1, 2, 3\n",
    "found_clusters = [wine.index[clusters == i] for i in range(3)]    # but kmeans will label them 0, 1, 2\n",
    "found_clusters.sort(key = lambda l: min(l))                     # kmeans doesn't order clusters\n",
    "colors = ['blue', 'orange', 'red']\n",
    "for i in range(3):\n",
    "    plt.plot(true_clusters[i], np.ones_like(true_clusters[i]), '.', color = colors[i])\n",
    "    plt.plot(found_clusters[i], 2 * np.ones_like(found_clusters[i]), '.', color = colors[i])\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('True labels' + 20 * ' ' + 'Found labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our $k$-means did an ok job of isolating one of the clusters, but totally mixed the other two.\n",
    "\n",
    "Is there anything we *didn't* do that might improve these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a plot to compare the true labels to the found labels\n",
    "ax = plt.figure(figsize = (12, 4))\n",
    "true_clusters = [wine.index[labels == i] for i in range(1,4)]     # in the data set labels are 1, 2, 3\n",
    "found_clusters = [wine.index[clusters == i] for i in range(3)]    # but kmeans will label them 0, 1, 2\n",
    "found_clusters.sort(key = lambda l: min(l))                     # kmeans doesn't order clusters\n",
    "colors = ['blue', 'orange', 'red']\n",
    "for i in range(3):\n",
    "    plt.plot(true_clusters[i], np.ones_like(true_clusters[i]), '.', color = colors[i])\n",
    "    plt.plot(found_clusters[i], 2 * np.ones_like(found_clusters[i]), '.', color = colors[i])\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('True labels' + 20 * ' ' + 'Found labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, much better.\n",
    "\n",
    "Don't forget to scale your inputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (14, 6))\n",
    "for i in range(3):\n",
    "    ax1.plot(wine.loc[found_clusters[i], 'Alcohol'], \n",
    "             wine.loc[found_clusters[i],'Malic Acid'],\n",
    "             'o', color = colors[i])\n",
    "ax1.set_xlabel('Alcohol')\n",
    "ax1.set_ylabel('Malic Acid')\n",
    "for i in range(3):\n",
    "    ax2.plot(wine.loc[found_clusters[i],'Ash'], \n",
    "             wine.loc[found_clusters[i],'Magnesium'],\n",
    "             'o', color = colors[i])\n",
    "ax2.set_xlabel('Ash')\n",
    "ax2.set_ylabel('Magnesium')\n",
    "for i in range(3):\n",
    "    ax3.plot(wine.loc[found_clusters[i],'Proanthocynanins'], \n",
    "             wine.loc[found_clusters[i], 'Color intensity'],\n",
    "             'o', color = colors[i])\n",
    "ax3.set_xlabel('Proanthocyanins')\n",
    "ax3.set_ylabel('Color intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
