{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to gradient descent and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(15) * 4 + 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mean as a minimization problem\n",
    "\n",
    "We talked about this back when we started doing curve fitting: the mean, the simplest model for a numerical variable, can be thought of as the solution to a least-squares minimization problem.\n",
    "\n",
    "Define the *cost function*\n",
    "\n",
    "$$ SSE(x^*; \\mathbf x) = \\sum_{i=0}^{m-1} (x_i - x^*)^2 $$\n",
    "\n",
    "(sum of squared errors). This is of course proportional to the variance of the data. We think of this as a function of the estimate $x^*$, not the data $\\mathbf x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function by calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Derivative:__ <font color = 'black'> An operation in calculus that calculates the rate of change (slope) of a function at a point. If we have a function $f(x)$, we can find a function $f'(x)$ with the property that the \"slope\" of the graph of $f$ at a specific point $x_0$ is $f'(x_0)$. This function is called the derivative of $f$.\n",
    "    \n",
    "There is a whole list of rules for calculating the derivative of a function if you have a formula for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Gradient:__ <font color = 'black'> For a function $f(\\mathbf x)$ that depends on several variables, the gradient is a vector built out of its derivative with respect to each of the variables. The important thing to know about the gradient is that as a vector, it points in the direction of greatest increase of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Critical point:__ <font color = 'black'> For any function $f(\\mathbf x)$, a critical point is a value of the input $\\mathbf x$ where the derivative/gradient is 0 (or undefined). A minimum/maximum value of $f$ must occur at a critical point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing a function iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Gradient descent:__ <font color = 'black'> An iterative approach to minimizing a function. *Iterative* means we start with a guess and then try to improve it. In gradient descent, we improve the guess by taking a small step in the direction of the (negative) gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_mean(data, candidate):\n",
    "    return sum(-2 * (x - candidate) for x in data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_mean(data, learn_rate = 0.01, tol = 0.0001, verbose = False):\n",
    "    # pick a random starting point\n",
    "    candidate = np.random.uniform(np.min(data), np.max(data))\n",
    "    delta = 1\n",
    "    while abs(delta) > tol:\n",
    "        grad = gradient_mean(data, candidate)\n",
    "        delta = learn_rate * grad\n",
    "        candidate -= delta\n",
    "        if verbose:\n",
    "            print(candidate)\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Learning rate:__ <font color = 'black'> The multiplier we apply to the gradient in gradient descent. Higher learning rate means we take bigger steps. In principle this means our optimizer should converge faster. But pushing it too high can cause problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent_path(data, learn_rate = 0.01, tol = 0.0001):\n",
    "    # pick a random starting point\n",
    "    candidate = np.random.uniform(np.min(data), np.max(data))\n",
    "    path = [candidate]\n",
    "    delta = 1\n",
    "    while abs(delta) > tol:\n",
    "        grad = gradient_mean(data, candidate)\n",
    "        delta = learn_rate * grad\n",
    "        candidate -= delta\n",
    "        path.append(candidate)\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = grad_descent_path(data)\n",
    "plt.plot(np.arange(1, path.shape[0] + 1), path, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "Notice that the gradient of the SSE is actually a sum of many terms, each of which depends on just one instance. This is a pretty common form for loss functions, because the loss measures the difference between the model prediction and the true value, and the simplest way to combine these differences is to just add them up (or average them, etc.).\n",
    "\n",
    "The problem is that if you have a really big data set, and if your gradient is more complicated to calculate than what we have above, it can be really computationally expensive to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Stochastic gradient descent:__ A variation on gradient descent where we estimate the gradient by using just a single example at a time. We shuffle the data set and then step through it, calculating the gradient one step at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_mean(data, learn_rate = 0.001, tol = 0.0001, verbose = False):\n",
    "    candidate = np.random.uniform(np.min(data), np.max(data))\n",
    "    delta = 1\n",
    "    temp_data = data.copy()\n",
    "    while abs(delta) > tol:\n",
    "        np.random.shuffle(temp_data)\n",
    "        for x in temp_data:\n",
    "            grad = -2*(x - candidate)\n",
    "            delta = grad * learn_rate\n",
    "            candidate -= delta\n",
    "            if verbose:\n",
    "                print(candidate)\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_mean(data, learn_rate = 0.001, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Learning rate decay:__ <font color = 'black'> Reducing the learning rate over time to prevent early stopping without causing the model to diverge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'green'> __Epoch:__ <font color = 'black'> A single pass through the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_mean_decay(data, learn_rate = 0.05, tol = 0.0001, verbose = False):\n",
    "    candidate = np.random.uniform(np.min(data), np.max(data))\n",
    "    delta = 1\n",
    "    temp_data = data.copy()\n",
    "    epoch = 1\n",
    "    while abs(delta) > tol:\n",
    "        np.random.shuffle(temp_data)\n",
    "        for x in temp_data:\n",
    "            grad = -2*(x - candidate) / len(data)\n",
    "            delta = grad * learn_rate / epoch ** (1/2)\n",
    "            candidate -= delta\n",
    "            if verbose:\n",
    "                print(candidate)\n",
    "        epoch += 1\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_mean_decay_path(data, learn_rate = 0.02, tol = 0.0001, verbose = False):\n",
    "    candidate = np.random.uniform(np.min(data), np.max(data))\n",
    "    delta = 1\n",
    "    temp_data = data.copy()\n",
    "    epoch = 1\n",
    "    path = [candidate]\n",
    "    while abs(delta) > tol:\n",
    "        np.random.shuffle(temp_data)\n",
    "        for x in temp_data:\n",
    "            grad = -2*(x - candidate)\n",
    "            delta = grad * learn_rate / epoch ** (1/2)\n",
    "            candidate -= delta\n",
    "            path.append(candidate)\n",
    "            if verbose:\n",
    "                print(candidate)\n",
    "        epoch += 1\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = sgd_mean_decay_path(data, learn_rate = 0.01)\n",
    "plt.plot(np.arange(1, path.shape[0] + 1), path, 'o')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
